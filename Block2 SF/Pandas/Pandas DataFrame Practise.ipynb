{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "student_data = pd.read_csv('data_test/students_performance.csv', sep = ',')\n",
    "\n",
    "#Информация о столбцах таблицы:\n",
    "student_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   gender                       1000 non-null   object\n",
      " 1   race/ethnicity               1000 non-null   object\n",
      " 2   parental level of education  1000 non-null   object\n",
      " 3   lunch                        1000 non-null   object\n",
      " 4   test preparation course      1000 non-null   object\n",
      " 5   math score                   1000 non-null   int64 \n",
      " 6   reading score                1000 non-null   int64 \n",
      " 7   writing score                1000 non-null   int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 62.6+ KB\n",
      "73\n",
      "70.03410852713178 58.92112676056338\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from xmlrpc.server import SimpleXMLRPCRequestHandler\n",
    "import pandas as pd\n",
    "student_data = pd.read_csv('data_test/students_performance.csv', sep = ',')\n",
    "\n",
    "#Информация о столбцах таблицы:\n",
    "student_data.head()\n",
    "\"\"\"gender — пол;\n",
    "race/ethnicity — раса/этническая принадлежность;\n",
    "parental level of education — уровень образования родителей;\n",
    "lunch — какие обеды получал студент во время обучения (standard — платный, free/reduced — бесплатный);\n",
    "test preparation course — посещал ли студент курсы подготовки к экзаменам (none — не посещал, completed — посещал);\n",
    "math score, reading score, writing score — баллы по математике, чтению и письму по сто балльной шкале.\"\"\"\n",
    "\n",
    "#Данные о скольких студентах содержатся в таблице?\n",
    "student_data.shape\n",
    "\n",
    "#Каков балл по письму у студента под индексом 155?\n",
    "student_data['writing score'].iloc[155] #\n",
    "student_data.loc[155,'writing score']\n",
    "\n",
    "#Сколько суммарно пропущенных значений в таблице?\n",
    "#Сколько столбцов в таблице имеет тип данных object?\n",
    "#Какой объём памяти (в килобайтах) занимает таблица?\n",
    "student_data.info()\n",
    "\n",
    "#Каков у студентов средний балл по математике?\n",
    "round (student_data['math score'].mean())\n",
    "\n",
    "#Какая расовая группа является самой крупной в учебном заведении?\n",
    "student_data['race/ethnicity'].mode()\n",
    "\n",
    "#Каков средний балл по чтению у студентов, которые посещали курсы подготовки к экзаменам?\n",
    "print (int (student_data[student_data['test preparation course'] == 'completed']['reading score'].mean()))\n",
    "\n",
    "#Сколько студентов получили 0 баллов по математике?\n",
    "student_data[student_data['math score'] == 0].shape[0]\n",
    "\n",
    "#Проверьте гипотезу: у студентов с оплачиваемым питанием средний балл по математике выше, \n",
    "#чем у студентов с льготным питанием.В качестве ответа напишите наибольший средний балл по математике среди этих групп студентов.\n",
    "standart_lunch_mean = student_data[student_data['lunch'] == 'standard']['math score'].mean()\n",
    "free_lunch_mean = student_data[student_data['lunch'] == 'free/reduced']['math score'].mean()\n",
    "print (standart_lunch_mean, free_lunch_mean)\n",
    "\n",
    "#Каков процент студентов, родители которых имеют высшее образование уровня бакалавриата (bachelor's degree)?\n",
    "round (student_data['parental level of education'].value_counts(normalize=True),2)*100\n",
    "\n",
    "#Насколько медианный балл по письму у студентов в расовой группе А отличается от среднего балла \n",
    "#по письму у студентов в расовой группе C?\n",
    "a = student_data[student_data['race/ethnicity'] == 'group A']['writing score'].median()\n",
    "c = student_data[student_data['race/ethnicity'] == 'group C']['writing score'].mean()\n",
    "print(round(abs(a - c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   starttime                300000 non-null  object \n",
      " 1   stoptime                 300000 non-null  object \n",
      " 2   start station id         299831 non-null  float64\n",
      " 3   start station name       299831 non-null  object \n",
      " 4   start station latitude   300000 non-null  float64\n",
      " 5   start station longitude  300000 non-null  float64\n",
      " 6   end station id           299831 non-null  float64\n",
      " 7   end station name         299831 non-null  object \n",
      " 8   end station latitude     300000 non-null  float64\n",
      " 9   end station longitude    300000 non-null  float64\n",
      " 10  bikeid                   300000 non-null  int64  \n",
      " 11  usertype                 300000 non-null  object \n",
      " 12  birth year               300000 non-null  int64  \n",
      " 13  gender                   300000 non-null  int64  \n",
      "dtypes: float64(6), int64(3), object(5)\n",
      "memory usage: 32.0+ MB\n",
      "False\n",
      "0 days 00:07:16.837000\n",
      "115135\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "citibike_data = pd.read_csv('data_test/citibike-tripdata.csv')\n",
    "citibike_df = citibike_data.copy()\n",
    "\"\"\"Датасет представляет собой таблицу с информацией о 300 тысячах поездок за первые пять дней сентября 2018 года \n",
    "и включает в себя следующую информацию:\n",
    "starttime — время начала поездки (дата, время);\n",
    "stoptime — время окончания поездки (дата, время);\n",
    "start station id — идентификатор стартовой стоянки;\n",
    "start station name — название стартовой стоянки;\n",
    "start station latitude, start station longitude — географическая широта и долгота стартовой стоянки;\n",
    "end station id — идентификатор конечной стоянки;\n",
    "end station name — название конечной стоянки;\n",
    "end station latitude, end station longitude — географическая широта и долгота конечной стоянки;\n",
    "bikeid — идентификатор велосипеда;\n",
    "usertype — тип пользователя (Customer — клиент с подпиской на 24 часа или на три дня, \n",
    "Subscriber — подписчик с годовой арендой велосипеда);\n",
    "birth year — год рождения клиента;\n",
    "gender — пол клиента (0 — неизвестный, 1 — мужчина, 2 — женщина).\"\"\"\n",
    "#Сколько пропусков в столбце start station id?\n",
    "citibike_df.info()\n",
    "#Найдите идентификатор самой популярной стартовой стоянки. Запишите идентификатор в виде целого числа.\n",
    "citibike_df['start station id'].mode()[0]\n",
    "#Велосипед с каким идентификатором является самым популярным?\n",
    "citibike_df['bikeid'].mode()[0]\n",
    "#Какой тип клиентов (столбец usertype) является преобладающим — Subscriber или Customer? В качестве ответа \n",
    "# запишите долю клиентов преобладающего типа среди общего количества клиентов. Ответ округлите до сотых.\n",
    "round (citibike_df['usertype'].value_counts(normalize=True),2)\n",
    "\"\"\"mode_usertype = citibike_df['usertype'].mode()[0]\n",
    "count_mode_user = citibike_df[citibike_df['usertype'] == mode_usertype].shape[0]\n",
    "print(round(count_mode_user / citibike_df.shape[0], 2))\"\"\"\n",
    "#Кто больше занимается велоспортом — мужчины или женщины? \n",
    "# В ответ запишите число поездок для той группы, у которой их больше.\n",
    "citibike_df['gender'].value_counts()\n",
    "\"\"\"male_count = citibike_df[citibike_df['gender'] == 1].shape[0]\n",
    "female_count = citibike_df[citybike_df['gender'] == 0].shape[0]\n",
    "print(max([male_count, female_count]))\"\"\"\n",
    "#Выберите утверждения,которые соответствуют нашим данным:\n",
    "#Число уникальных стартовых и конечных стоянок, которыми воспользовались клиенты, не равны друг другу\n",
    "if citibike_df['start station name'].nunique() == citibike_df['end station name'].nunique():\n",
    "    print (True)\n",
    "else:\n",
    "    print (False)\n",
    "#В рассматриваемые дни минимальный возраст клиента составлял 10 лет\n",
    "2018 - max (citibike_df['birth year'])\n",
    "#Самой непопулярной стартовой стоянкой из тех, которыми воспользовались клиенты, \n",
    "# является стоянка с названием \"Eastern Pkwy & Washington Ave\"\n",
    "citibike_df['start station name'].value_counts()\n",
    "#Наибольшее количество поездок завершается на стоянке под названием \"Liberty Light Rail\"\n",
    "citibike_df['end station name'].mode()\n",
    "#В первую очередь удалим лишнюю информацию из данных.\n",
    "#В наших данных присутствуют столбцы, которые дублируют информацию друг о друге: это столбцы с идентификатором \n",
    "#и названием стартовой и конечной стоянки. Удалите признаки идентификаторов стоянок. Сколько столбцов осталось?\n",
    "citibike_df = citibike_df.drop (['start station id'],axis=1)\n",
    "citibike_df = citibike_df.drop (['end station id'],axis=1)\n",
    "citibike_df.shape[1]\n",
    "\"\"\"citibike_df.drop(['start station id', 'end station id'], axis=1, inplace=True)\n",
    "print(citiboke_df.shape[1])\"\"\"\n",
    "#Замените признак birth year на более понятный признак возраста клиента age. Годом отсчёта возраста \n",
    "# выберите 2018 год. Столбец birth year удалите из таблицы. Сколько поездок совершено клиентами старше 60 лет?\n",
    "citibike_df['age'] = 2018 - citibike_df['birth year']\n",
    "citibike_df = citibike_df.drop (['birth year'],axis=1)\n",
    "sum (citibike_df['age'] > 60)\n",
    "\"\"\"citibike_df['age'] = 2018 - citibike_df['birth year']\n",
    "citibike_df.drop(['birth year'], axis=1, inplace=True)\n",
    "print(citibike_df[citibike_df['age'] > 60].shape[0])\"\"\"\n",
    "#Создайте признак длительности поездки trip duration. Для этого вычислите интервал времени между временем окончания \n",
    "# поездки (stoptime) и её началом (starttime). Сколько целых минут длилась поездка под индексом 3 в таблице?\n",
    "citibike_df['starttime'] = pd.to_datetime(citibike_df['starttime'])\n",
    "citibike_df['stoptime'] = pd.to_datetime(citibike_df['stoptime'])\n",
    "citibike_df['trip_duration'] = citibike_df['stoptime'] - citibike_df['starttime']\n",
    "print (citibike_df['trip_duration'].iloc[3])  #citibike_df.loc[3,'trip_duration']\n",
    "#Создайте «признак-мигалку» weekend, который равен 1, если поездка начиналась в выходной день (суббота или \n",
    "# воскресенье), и 0 — в противном случае. Выясните, сколько поездок начиналось в выходные.\n",
    "citibike_df['weekend_starttime'] = citibike_df['starttime'].dt.day\n",
    "weekend_starttime = citibike_df[(citibike_df['weekend_starttime'] == 1) | (citibike_df['weekend_starttime'] == 2)].shape[0]\n",
    "print (weekend_starttime)\n",
    "\"\"\"weekday = citibike_df['starttime'].dt.dayofweek\n",
    "citibike_df['weekend'] = weekday.apply(lambda x: 1 if x == 5 or x == 6 else 0)\n",
    "citibike_df['weekend'].sum()\"\"\"\n",
    "#Создайте признак времени суток поездки time_of_day. Время суток будем определять из часа начала поездки. \n",
    "# Условимся, что:\n",
    "#поездка совершается ночью (night), если её час приходится на интервал от 0 (включительно) до 6 (включительно) часов;\n",
    "#поездка совершается утром (morning), если её час приходится на интервал от 6 (не включительно) \n",
    "# до 12 (включительно) часов;\n",
    "#поездка совершается днём (day), если её час приходится на интервал от 12 (не включительно) \n",
    "# до 18 (включительно) часов;\n",
    "#поездка совершается вечером (evening), если её час приходится на интервал от 18 (не включительно) \n",
    "# до 23 часов (включительно).\n",
    "#Во сколько раз количество поездок, совершённых днём, больше, чем количество поездок, совёршенных ночью, \n",
    "# за представленный в данных период времени? Ответ округлите до целых.\n",
    "\"\"\"citi_daytime = citibike_df['starttime'].dt.time\n",
    "#print (citi_daytime.iloc[00:00:27])\n",
    "citi_daytime = citi_daytime.apply (lambda x: 'night' if x <= 06:00:00 else x)\"\"\"\n",
    "\n",
    "def get_per_day(time):\n",
    "    if 0 <= time <= 6:\n",
    "        return 'night'\n",
    "    if 6 < time <= 12:\n",
    "        return 'morning'\n",
    "    if 12 < time <= 18:\n",
    "        return 'day'\n",
    "    if 18 < time <= 23:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'else'\n",
    "\n",
    "citibike_df['daytime'] = citibike_df['starttime'].dt.hour.apply(get_per_day)\n",
    "a = citibike_df[citibike_df['daytime'] == 'day'].shape[0]\n",
    "b = citibike_df[citibike_df['daytime'] == 'night'].shape[0]\n",
    "print (round (a/b))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Address'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Address'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         street_type \u001b[39m=\u001b[39m address_list [\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m street_type\n\u001b[1;32m---> 13\u001b[0m street_types \u001b[39m=\u001b[39m melb_df[\u001b[39m'\u001b[39;49m\u001b[39mAddress\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(get_street_type)\n\u001b[0;32m     14\u001b[0m street_types\u001b[39m.\u001b[39mvalue_counts()\n\u001b[0;32m     15\u001b[0m refind_str_types \u001b[39m=\u001b[39m street_types\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39mBvd\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBoulevard\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m x)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Address'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "melb_data = pd.read_csv ('data_test/melb_data.csv')\n",
    "melb_df = melb_data.copy()\n",
    "melb_df.head(8)\n",
    "def get_street_type(address):\n",
    "    exclude_list = ['N','S','W','E']\n",
    "    address_list = address.split(' ')\n",
    "    street_type = address_list[-1]\n",
    "    if street_type in exclude_list:\n",
    "        street_type = address_list [-2]\n",
    "    return street_type\n",
    "\n",
    "street_types = melb_df['Address'].apply(get_street_type)\n",
    "street_types.value_counts()\n",
    "refind_str_types = street_types.apply(lambda x: \"Bvd\" if x == 'Boulevard' else x)\n",
    "refind_str_types = street_types.apply(lambda x: \"Pde\" if x == 'Parade' else x)\n",
    "refind_str_types = street_types.apply(lambda x: \"Av\" if x == 'Avenue' else x)\n",
    "refind_str_types.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Eastern Metropolitan</th>\n",
       "      <th>Eastern Victoria</th>\n",
       "      <th>Northern Metropolitan</th>\n",
       "      <th>Northern Victoria</th>\n",
       "      <th>South-Eastern Metropolitan</th>\n",
       "      <th>Southern Metropolitan</th>\n",
       "      <th>Western Metropolitan</th>\n",
       "      <th>Western Victoria</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">PI</th>\n",
       "      <th>house</th>\n",
       "      <td>1244000</td>\n",
       "      <td>780000</td>\n",
       "      <td>900000</td>\n",
       "      <td>500000</td>\n",
       "      <td>865000</td>\n",
       "      <td>1725000</td>\n",
       "      <td>870000</td>\n",
       "      <td>630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townhouse</th>\n",
       "      <td>760000</td>\n",
       "      <td>0</td>\n",
       "      <td>632500</td>\n",
       "      <td>0</td>\n",
       "      <td>1190000</td>\n",
       "      <td>1055000</td>\n",
       "      <td>670000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>650000</td>\n",
       "      <td>0</td>\n",
       "      <td>410000</td>\n",
       "      <td>0</td>\n",
       "      <td>525000</td>\n",
       "      <td>571250</td>\n",
       "      <td>360000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">S</th>\n",
       "      <th>house</th>\n",
       "      <td>1127000</td>\n",
       "      <td>675000</td>\n",
       "      <td>920000</td>\n",
       "      <td>555000</td>\n",
       "      <td>883300</td>\n",
       "      <td>1611000</td>\n",
       "      <td>870000</td>\n",
       "      <td>397500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townhouse</th>\n",
       "      <td>828000</td>\n",
       "      <td>0</td>\n",
       "      <td>750000</td>\n",
       "      <td>0</td>\n",
       "      <td>875000</td>\n",
       "      <td>1135000</td>\n",
       "      <td>729000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>645750</td>\n",
       "      <td>492000</td>\n",
       "      <td>525500</td>\n",
       "      <td>0</td>\n",
       "      <td>606000</td>\n",
       "      <td>655000</td>\n",
       "      <td>489000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SA</th>\n",
       "      <th>house</th>\n",
       "      <td>932500</td>\n",
       "      <td>950000</td>\n",
       "      <td>817500</td>\n",
       "      <td>540000</td>\n",
       "      <td>880000</td>\n",
       "      <td>1390000</td>\n",
       "      <td>772500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townhouse</th>\n",
       "      <td>807500</td>\n",
       "      <td>0</td>\n",
       "      <td>425000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1141000</td>\n",
       "      <td>467500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>616000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>580000</td>\n",
       "      <td>571000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SP</th>\n",
       "      <th>house</th>\n",
       "      <td>1050000</td>\n",
       "      <td>672500</td>\n",
       "      <td>900000</td>\n",
       "      <td>521000</td>\n",
       "      <td>770000</td>\n",
       "      <td>1521750</td>\n",
       "      <td>865000</td>\n",
       "      <td>360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townhouse</th>\n",
       "      <td>910000</td>\n",
       "      <td>0</td>\n",
       "      <td>690000</td>\n",
       "      <td>0</td>\n",
       "      <td>800000</td>\n",
       "      <td>1162500</td>\n",
       "      <td>702500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>515000</td>\n",
       "      <td>400000</td>\n",
       "      <td>470000</td>\n",
       "      <td>0</td>\n",
       "      <td>601000</td>\n",
       "      <td>550000</td>\n",
       "      <td>460000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">VB</th>\n",
       "      <th>house</th>\n",
       "      <td>1100000</td>\n",
       "      <td>712500</td>\n",
       "      <td>1050000</td>\n",
       "      <td>690000</td>\n",
       "      <td>850000</td>\n",
       "      <td>1800000</td>\n",
       "      <td>880000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townhouse</th>\n",
       "      <td>892500</td>\n",
       "      <td>0</td>\n",
       "      <td>640000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1250000</td>\n",
       "      <td>689500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>500000</td>\n",
       "      <td>0</td>\n",
       "      <td>450000</td>\n",
       "      <td>0</td>\n",
       "      <td>700000</td>\n",
       "      <td>500000</td>\n",
       "      <td>420000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Regionname        Eastern Metropolitan  Eastern Victoria  \\\n",
       "Method Type                                                \n",
       "PI     house                   1244000            780000   \n",
       "       townhouse                760000                 0   \n",
       "       unit                     650000                 0   \n",
       "S      house                   1127000            675000   \n",
       "       townhouse                828000                 0   \n",
       "       unit                     645750            492000   \n",
       "SA     house                    932500            950000   \n",
       "       townhouse                807500                 0   \n",
       "       unit                          0                 0   \n",
       "SP     house                   1050000            672500   \n",
       "       townhouse                910000                 0   \n",
       "       unit                     515000            400000   \n",
       "VB     house                   1100000            712500   \n",
       "       townhouse                892500                 0   \n",
       "       unit                     500000                 0   \n",
       "\n",
       "Regionname        Northern Metropolitan  Northern Victoria  \\\n",
       "Method Type                                                  \n",
       "PI     house                     900000             500000   \n",
       "       townhouse                 632500                  0   \n",
       "       unit                      410000                  0   \n",
       "S      house                     920000             555000   \n",
       "       townhouse                 750000                  0   \n",
       "       unit                      525500                  0   \n",
       "SA     house                     817500             540000   \n",
       "       townhouse                 425000                  0   \n",
       "       unit                      616000                  0   \n",
       "SP     house                     900000             521000   \n",
       "       townhouse                 690000                  0   \n",
       "       unit                      470000                  0   \n",
       "VB     house                    1050000             690000   \n",
       "       townhouse                 640000                  0   \n",
       "       unit                      450000                  0   \n",
       "\n",
       "Regionname        South-Eastern Metropolitan  Southern Metropolitan  \\\n",
       "Method Type                                                           \n",
       "PI     house                          865000                1725000   \n",
       "       townhouse                     1190000                1055000   \n",
       "       unit                           525000                 571250   \n",
       "S      house                          883300                1611000   \n",
       "       townhouse                      875000                1135000   \n",
       "       unit                           606000                 655000   \n",
       "SA     house                          880000                1390000   \n",
       "       townhouse                           0                1141000   \n",
       "       unit                                0                 580000   \n",
       "SP     house                          770000                1521750   \n",
       "       townhouse                      800000                1162500   \n",
       "       unit                           601000                 550000   \n",
       "VB     house                          850000                1800000   \n",
       "       townhouse                           0                1250000   \n",
       "       unit                           700000                 500000   \n",
       "\n",
       "Regionname        Western Metropolitan  Western Victoria  \n",
       "Method Type                                               \n",
       "PI     house                    870000            630000  \n",
       "       townhouse                670000                 0  \n",
       "       unit                     360000                 0  \n",
       "S      house                    870000            397500  \n",
       "       townhouse                729000                 0  \n",
       "       unit                     489000                 0  \n",
       "SA     house                    772500                 0  \n",
       "       townhouse                467500                 0  \n",
       "       unit                     571000                 0  \n",
       "SP     house                    865000            360000  \n",
       "       townhouse                702500                 0  \n",
       "       unit                     460000                 0  \n",
       "VB     house                    880000                 0  \n",
       "       townhouse                689500                 0  \n",
       "       unit                     420000                 0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melb_df = pd.read_csv('data_test/melb_data_fe.csv')\n",
    "\n",
    "melb_df.sort_values(\n",
    "    by=['Date', 'Regionname', 'Rooms'],\n",
    "    ascending=[True, True, False],\n",
    "    ignore_index=True\n",
    ").loc[::10, ['Date', 'Regionname', 'Rooms']].head(10)\n",
    "\n",
    "melb_df.groupby(by='Type')['Price'].mean()\n",
    "#Remoteness by regions\n",
    "melb_df.groupby(by='Regionname', as_index=False)['Distance'].min().sort_values(\n",
    "    by='Distance', ascending=False)\n",
    "#to calculate several aggregating methods, we will use the method agg()\n",
    "melb_df.groupby('MonthSale')['Price'].agg(\n",
    "    ['count','mean','max']\n",
    ").sort_values(by='count', ascending=False)\n",
    "#method agg() support use of another functions\n",
    "melb_df.groupby('Regionname')['SellerG'].agg(\n",
    "    ['nunique', set]\n",
    ")\n",
    "\n",
    "#summary tables defaults to average\n",
    "melb_df.pivot_table(\n",
    "    values='Price',\n",
    "    index='Rooms',\n",
    "    columns='Type',\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "#for several parameters need to use argument aggfunc=['with list of agregated functions']\n",
    "melb_df.pivot_table(\n",
    "    values='Landsize',\n",
    "    index='Regionname',\n",
    "    columns='Type',\n",
    "    aggfunc=['mean','median'],\n",
    "    fill_value=0\n",
    ")               \n",
    "#summary tables allow to consider the dependence on a larger number of signs, \n",
    "#we can pass a list of features to a parameter \"index\" or \"columns\"\n",
    "melb_df.pivot_table(\n",
    "    values='Price',\n",
    "    index=['Method', 'Type'],\n",
    "    columns='Regionname',\n",
    "    aggfunc='median',\n",
    "    fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  userId  movieId  rating                 date  \\\n",
      "0           0       1        1     4.0  2000-07-30 18:45:03   \n",
      "1           1       1        3     4.0  2000-07-30 18:20:47   \n",
      "2           2       1        6     4.0  2000-07-30 18:37:04   \n",
      "3           3       1       47     5.0  2000-07-30 19:03:35   \n",
      "4           4       1       50     5.0  2000-07-30 18:48:51   \n",
      "\n",
      "                         title                                       genres  \n",
      "0             Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1      Grumpier Old Men (1995)                               Comedy|Romance  \n",
      "2                  Heat (1995)                        Action|Crime|Thriller  \n",
      "3  Seven (a.k.a. Se7en) (1995)                             Mystery|Thriller  \n",
      "4   Usual Suspects, The (1995)                       Crime|Mystery|Thriller  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "year_rating\n",
       "1999    3.606061\n",
       "2013    3.489474\n",
       "2012    3.478477\n",
       "2008    3.426667\n",
       "1997    3.409091\n",
       "2016    3.373431\n",
       "2004    3.356877\n",
       "2006    3.347534\n",
       "2001    3.318408\n",
       "2009    3.273292\n",
       "2014    3.271429\n",
       "2011    3.232877\n",
       "1996    3.228571\n",
       "2002    3.198556\n",
       "2010    3.179825\n",
       "2000    3.141291\n",
       "2018    3.121296\n",
       "2003    3.120066\n",
       "2015    3.106183\n",
       "1998    3.000000\n",
       "2005    2.963325\n",
       "2007    2.928187\n",
       "2017    2.852668\n",
       "Name: Comedy, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies_df = pd.read_csv('data_test/ratings_movies.csv')\n",
    "print (movies_df.head())\n",
    "#Для решения задач нам понадобится выделить из признака title год выпуска фильма. \n",
    "#Для этого напишем функцию get_year_release(arg).\n",
    "#библиотека для регулярных выражений\n",
    "import re \n",
    "def get_year_release(arg):\n",
    "    #находим все слова по шаблону \"(DDDD)\"\n",
    "    candidates = re.findall(r'\\(\\d{4}\\)', arg) \n",
    "    # проверяем число вхождений\n",
    "    if len(candidates) > 0:\n",
    "        #если число вхождений больше 0,\n",
    "\t#очищаем строку от знаков \"(\" и \")\"\n",
    "        year = candidates[0].replace('(', '')\n",
    "        year = year.replace(')', '')\n",
    "        return int(year)\n",
    "    else:\n",
    "        #если год не указан, возвращаем None\n",
    "        return None\n",
    "\n",
    "#Создайте в таблице новый признак year_release, который соответствует году выпуска фильма.\n",
    "#У скольких фильмов не указан год их выпуска?\n",
    "movies_df['year_release'] = movies_df['title'].apply(get_year_release)\n",
    "#Для начала обратимся к методам из библиотеки pandas, которые позволяют быстро определить наличие\n",
    "#элементов NaN в структурах. Если таблица небольшая, то можно использовать библиотечный метод isnull\n",
    "movies_df['year_release'].isnull().sum()\n",
    "#Отсутствующие данные объектов можно заменить на конкретные числовые значения, для этого можно\n",
    "#использовать метод fillna()\n",
    "movies_df.fillna(0)\n",
    "#movies_df['year_release'] = movies_df['year_release'].astype('int32')\n",
    "#movies_df.info()\n",
    "#Какой фильм, выпущенный в 1999 году, получил наименьшую среднюю оценку зрителей?\n",
    "#В качестве ответа запишите название этого фильма без указания года его выпуска.\n",
    "#movies_df[movies_df['year_release'] == 1999]['rating'].mean()\n",
    "mask = movies_df['year_release'] == 1999\n",
    "films = movies_df[mask].groupby('title')['rating'].mean().sort_values()\n",
    "#films.iloc[0].index[0]#how to get film title?!\n",
    "\n",
    "#Какое сочетание жанров фильмов (genres), выпущенных в 2010 году, получило наименьшую среднюю оценку (rating)?\n",
    "movies_df[movies_df['year_release'] == 2010].groupby('genres')['rating'].mean().sort_values()\n",
    "#Какой пользователь(userId)посмотрел наибольшее количество различных(уникальных)комбинаций жанров(genres)фильмов?\n",
    "movies_df.groupby('userId')['genres'].nunique().sort_values(ascending=False)\n",
    "#Найдите пользователя, который выставил наименьшее количество оценок, но его средняя оценка фильмам наибольшая.\n",
    "#Чтобы рассчитать несколько параметров для каждого пользователя (количество оценок и среднюю оценку),\n",
    "#можно воспользоваться методом agg() на сгруппированных данных.\n",
    "movies_df.groupby('userId')['rating'].agg(\n",
    "    ['count','mean']\n",
    ").sort_values(['count', 'mean'], ascending=[True, False])\n",
    "#Найдите сочетание жанров (genres) за 2018 году, которое имеет наибольший средний рейтинг (среднее по столбцу\n",
    "#rating), и при этом число выставленных ему оценок (количество значений в столбце rating) больше 10.\n",
    "mask = movies_df['year_release'] == 2018\n",
    "grouped = movies_df[mask].groupby('genres')['rating'].agg(\n",
    "    ['mean','count']\n",
    ").sort_values(by='mean', ascending=False)\n",
    "grouped[grouped['count'] > 10]\n",
    "#Добавьте в таблицу новый признак year_rating — год выставления оценки. Создайте сводную таблицу, которая\n",
    "#иллюстрирует зависимость среднего рейтинга фильма от года выставления оценки и жанра.\n",
    "#movies_df['date'] = pd.to_datetime(movies_df['date'])\n",
    "movies_df['year_rating'] = pd.to_datetime(movies_df['date']).dt.year\n",
    "piv_movie = movies_df.pivot_table(\n",
    "    values='rating',\n",
    "    index='year_rating',\n",
    "    columns='genres',\n",
    "    fill_value=0\n",
    ")\n",
    "piv_movie\n",
    "#За весь период (с 1996 по 2018 год) сочетание жанров Action|Adventure ни разу не получало среднюю оценку ниже 3.\n",
    "piv_movie['Action|Adventure'].loc[1996:2018] >= 3\n",
    "#B Наилучшую оценку жанр Action|Adventure|Animation|Children|Comedy|IMAX получил в 2010 году.\n",
    "filtered = piv_movie['Action|Adventure|Animation|Children|Comedy|IMAX'][2010]\n",
    "max(piv_movie['Action|Adventure|Animation|Children|Comedy|IMAX']) == filtered\n",
    "#Среди сочетаний жанров, получивших наивысшую среднюю оценку в 2018 году, есть сочетание Animation|Children|Mystery.\n",
    "max(piv_movie.loc[2018]) == piv_movie.loc[2018]['Animation|Children|Mystery']\n",
    "#Для жанра Comedy прослеживается тенденция падения рейтинга с каждым годом (с 1996 по 2018).\n",
    "piv_movie['Comedy'].loc[1996:2018].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m orders_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata_test/orders.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m orders_df\n\u001b[1;32m----> 5\u001b[0m product_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata_test/products.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39m#Объедините заданные таблицы в таблицу orders_products, чтобы в результирующей таблице оказалась информация\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m#обо всех заказах, но не оказалось информации о продуктах, на которых заказов ещё не поступало. \u001b[39;00m\n\u001b[0;32m      9\u001b[0m orders_products \u001b[39m=\u001b[39m orders_df\u001b[39m.\u001b[39mmerge(\n\u001b[0;32m     10\u001b[0m     product_df, \n\u001b[0;32m     11\u001b[0m     left_on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mID товара\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     right_on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProduct_ID\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1772\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1765\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1766\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1767\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m     (\n\u001b[0;32m   1769\u001b[0m         index,\n\u001b[0;32m   1770\u001b[0m         columns,\n\u001b[0;32m   1771\u001b[0m         col_dict,\n\u001b[1;32m-> 1772\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1773\u001b[0m         nrows\n\u001b[0;32m   1774\u001b[0m     )\n\u001b[0;32m   1775\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:243\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 243\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    244\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orders_df = pd.read_csv('data_test/orders.csv')\n",
    "orders_df\n",
    "product_df = pd.read_csv('data_test/products.csv')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"#Объедините заданные таблицы в таблицу orders_products, чтобы в результирующей таблице оказалась информация\n",
    "#обо всех заказах, но не оказалось информации о продуктах, на которых заказов ещё не поступало.\n",
    "orders_products = orders_df.merge(\n",
    "    product_df, \n",
    "    left_on='ID товара',\n",
    "    right_on='Product_ID',\n",
    "    how='left')\n",
    "orders_products.tail(1)['Order ID']\n",
    "#На какой товар была произведена отмена?\n",
    "#В качестве ответа запишите название этого товара (Name).\n",
    "orders_products[orders_products['Отменен'] == 'Да']['Name']\n",
    "#Какой покупатель принёс наибольшую суммарную прибыль интернет-магазину за указанный период?\n",
    "#В ответ запишите идентификатор этого покупателя (ID Покупателя). Прибыль состоит только из оплаченных заказов\n",
    "#и рассчитывается как количество купленного товара, умноженное на его цену\n",
    "orders_products['Profit'] = orders_products['Price'] * orders_products['Количество'] \n",
    "orders_products[orders_products['Оплачен'] == 'Да'].groupby('ID Покупателя')['Profit'].sum().sort_values(ascending=False)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "no\n",
      "no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Noah', 'Emma'),\n",
       " ('Liam', 'Olivia'),\n",
       " ('Mason', 'Sophia'),\n",
       " ('Jacob', 'Isabella'),\n",
       " ('William', 'Ava'),\n",
       " ('Ethan', 'Mia'),\n",
       " ('Michael', 'Emily')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#Сначала попробуем вытащить каждый символ (используя .)\n",
    "result = re.findall(r'.', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Для того, чтобы в конечный результат не попал пробел, используем вместо . \\w.\n",
    "result = re.findall(r'\\w', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Теперь попробуем достать каждое слово (используя * или +)\n",
    "result = re.findall(r'\\w*', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#И снова в результат попали пробелы, так как * означает «ноль или более символов». \n",
    "#Для того, чтобы их убрать, используем +:\n",
    "result = re.findall(r'\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Теперь вытащим первое слово, используя ^:\n",
    "result = re.findall(r'^\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Если мы используем $ вместо ^, то мы получим последнее слово, а не первое:\n",
    "result = re.findall(r'\\w+$', 'AV is largest Analytics community of India')\n",
    "result\n",
    "\"\"\"Вернуть первые два символа каждого слова\"\"\"\n",
    "#Вариант 1: используя \\w, вытащить два последовательных символа, кроме пробельных, из каждого слова:\n",
    "result = re.findall(r'\\w\\w', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Вариант 2: вытащить два последовательных символа, используя символ границы слова (\\b):\n",
    "result = re.findall(r'\\b\\w.', 'AV is largest Analytics community of India')\n",
    "result\n",
    "\"\"\"Вернуть домены из списка email-адресов\"\"\"\n",
    "#Сначала вернём все символы после «@»:\n",
    "result = re.findall(r'@\\w+',\n",
    "            'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')\n",
    "result\n",
    "#Как видим, части «.com», «.in» и т. д. не попали в результат. Изменим наш код:\n",
    "result = re.findall(r'@\\w+.\\w+',\n",
    "            'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')\n",
    "result\n",
    "#Второй вариант — вытащить только домен верхнего уровня, используя группировку — ( ):\n",
    "result = re.findall(r'@\\w+.(\\w+)',\n",
    "            'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')\n",
    "result\n",
    "\"\"\"Извлечь дату из строки\"\"\"\n",
    "#Используем \\d для извлечения цифр.\n",
    "result = re.findall(r'\\d{2}-\\d{2}-\\d{4}',\n",
    "            'Amit 34-3456 12-05-2007, XYZ 56-4532 11-11-2011, ABC 67-8945 12-01-2009')\n",
    "result\n",
    "#Для извлечения только года нам опять помогут скобки:\n",
    "result = re.findall(r'\\d{2}-\\d{2}-(\\d{4})',\n",
    "            'Amit 34-3456 12-05-2007, XYZ 56-4532 11-11-2011, ABC 67-8945 12-01-2009')\n",
    "result\n",
    "\"\"\"Извлечь слова, начинающиеся на гласную\"\"\"\n",
    "#Для начала вернем все слова:\n",
    "result = re.findall(r'\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#А теперь — только те, которые начинаются на определенные буквы (используя []):\n",
    "result = re.findall(r'[aeiouAEIOU]\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Выше мы видим обрезанные слова «argest» и «ommunity». Для того, чтобы убрать их,\n",
    "#используем \\b для обозначения границы слова:\n",
    "result = re.findall(r'\\b[aeiouAEIOU]\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#Также мы можем использовать ^ внутри квадратных скобок для инвертирования группы:\n",
    "result = re.findall(r'\\b[^aeiouAEIOU]\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "#В результат попали слова, «начинающиеся» с пробела.\n",
    "#Уберем их, включив пробел в диапазон в квадратных скобках:\n",
    "result = re.findall(r'\\b[^aeiouAEIOU ]\\w+', 'AV is largest Analytics community of India')\n",
    "result\n",
    "\"\"\"Проверить формат телефонного номера\"\"\"\n",
    "#Номер должен быть длиной 10 знаков и начинаться с 8 или 9. Есть список телефонных номеров,\n",
    "#и нужно проверить их, используя регулярки в Python:\n",
    "li = ['9999999999', '999999-999', '99999x9999']\n",
    "\n",
    "for val in li:\n",
    "    if re.match(r'[8-9]{1}[0-9]{9}', val) and len(val) == 10:\n",
    "        print ('yes')\n",
    "    else:\n",
    "        print ('no')\n",
    "\n",
    "\"\"\"Разбить строку по нескольким разделителям\"\"\"\n",
    "#Возможное решение:\n",
    "line = 'asdf fjdk;afed,fjek,asdf,foo' # String has multiple delimiters (\";\",\",\",\" \").\n",
    "result = re.split(r'[;,\\s]', line)\n",
    "result\n",
    "#Также мы можем использовать метод re.sub() для замены всех разделителей пробелами:\n",
    "line = 'asdf fjdk;afed,fjek,asdf,foo'\n",
    "result = re.sub(r'[;,\\s]',' ', line)\n",
    "result\n",
    "\"\"\"Извлечь информацию из html-файла\"\"\"\n",
    "#Допустим, нужно извлечь информацию из html-файла, заключенную между <td> и </td>, кроме первого\n",
    "#столбца с номером. Также будем считать, что html-код содержится в строке.\n",
    "#Пример содержимого html-файла:\n",
    "test_str = '1NoahEmma2LiamOlivia3MasonSophia4JacobIsabella5WilliamAva6EthanMia7MichaelEmily'\n",
    "#С помощью регулярных выражений в Python это можно решить так (если поместить содержимое\n",
    "#файла в переменную test_str):\n",
    "result = re.findall(r'\\d([A-Z][A-Za-z]+)([A-Z][A-Za-z]+)', test_str)\n",
    "result\n",
    "\"\"\"Не нашел в статье информацию по моменту с методом findall, используя который если вхождения\n",
    "пересекаются, метод findall не возвращает второе вхождение, так как часть него включает часть \n",
    "первого вхождения. Пример: по паттерну условно подходят пары слов со словами под номерами 5и6 и\n",
    "6и7,метод возвращает только пару слов 5и6, игнорируя пару 6и7, после чего дальше идёт по строке.\n",
    "Есть ли какая возможность методом re обойти данный момент?\n",
    "\n",
    "Можно использовать стороннюю библиотеку — regex, которая умеет работать в режиме overlapping=True.\n",
    "\n",
    "Помогите, пожалуйста. Не могу нормально распарсить строку. Допустим, есть строка “field : \n",
    "test@gmail.com ; field : test2@gmail.com ; field : test3@gmail.com” Грубо говоря, мне нужно\n",
    "получить две переменные, чтобы в первой было “field”, а в другой “test@gmail.com” .\n",
    "\n",
    "модернизировал регулярку Влада: import re string = \"field1:test@gmail.com , field2:test2@gmail.com,\n",
    "field3:test3@gmail.com\" result3 = re.findall(r'(\\w+)\\W+(\\w+@\\w+.\\w+)', string) print(result3)\n",
    "\n",
    "import re result = re.findall(r'(\\w+@\\w+.\\w+)',\"field : test@gmail.com , field : test2@gmail.com ,\n",
    "field : test3@gmail.com\") print(result) result2 =re.findall(r'(\\w+)\\W+\\w+@\\w+.\\w+',\"field :\n",
    "test@gmail.com , field : test2@gmail.com , field : test3@gmail.com\") print(result2)\n",
    "\n",
    "А как можно, к примеру, вывести все числа, от 0 до 11 включительно?\n",
    "r'[01]{,2}\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
